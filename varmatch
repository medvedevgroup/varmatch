#!/usr/bin/env python

# Copyright 2015, Chen Sun
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""
    Authors:
    Chen Sun(chensun@cse.psu.edu)
    Paul Medvedev(pashadag@cse.psu.edu)
"""

import sys
versionError = "You are using an old version of python, please upgrade to python 2.7+\n"
if sys.hexversion < 0x02070000:
    print (versionError)
    exit()
import textwrap as _textwrap
import multiprocessing
import argparse
import os
import subprocess
import time

RUN = True

author_email = 'chensun@cse.psu.edu'

class SmartFormatter(argparse.HelpFormatter):
    def _split_lines(self, text, width):
        paragraphs = text.split('\n')
        #return paragraphs
        multiline_text = []
        for paragraph in paragraphs:
            formatted_paragraph = _textwrap.wrap(paragraph, width)
            multiline_text = multiline_text + formatted_paragraph
        return multiline_text

    def _fill_text(self, text, width, indent):
        return ''.join(indent + line for line in text.splitlines(True))

citation = 'Please cite our paper.'

parser = argparse.ArgumentParser(prog="varmatch", epilog = citation, formatter_class=lambda prog: SmartFormatter(prog,max_help_position=8))
parser.add_argument('-b', '--baseline', required=True, metavar='File', help = 'baseline variant VCF filename')
parser.add_argument('-q', '--query', nargs='+', metavar='File List', help = 'query variant VCF filename')
parser.add_argument('-g', '--genome', required=True, metavar='File', help= 'genome sequence FASTA filename')
parser.add_argument('-o', '--output', help='output directory', metavar='DIRECTORY',default='./output')

thread_string = "number of threads, default is the number of available cores (For this machine:" + str(multiprocessing.cpu_count()) + \
                ")\nIf larger than number of available cores or less than 1, automatically set to default value"

parser.add_argument('-t', '--thread', metavar="INT", help=thread_string, default=str(multiprocessing.cpu_count()))

score_unit_string = "scoring function/score unit: (Default: -1)\n"\
    "-1 : iterate both 0 and 1.\n"\
    "0  : the score that a VCF entry contributes is 1.\n"\
    "1  : the score that a VCF entry contributes is the edit distance between the new allele and the reference one.\n"

match_mode_string = "matching mode: (Default: -1)\n"\
    "-1 : iterate both 0 and 1.\n"\
    "0  : a set of query entries match a set of baseline entries if, "\
    "for each entry, we can select one of the alleles such that the inferred sequences are identical\n"\
    "1  : a set of query entries match a set of baseline entries if there exist a phasing of each set such that "\
    "the two inferred haplotypes from the query are equal to the two inferred haplotypes from the baseline.\n"

score_scheme_string = "scoring scheme: (Default: -1)\n"\
    "-1 : iterate 0, 1, and 2 (excluding 3)\n"\
    "0  : find two subsets of non-overlapping equivalent variants such that "\
    "the score of the matched variants is maximized \n"\
    "1  : find two subsets of non-overlapping equivalent variants such that"\
    " the score of the chosen baseline variants is maximized\n"\
    "2  : find a maximum scoring set of variants in the query such that"\
    " each variant can be matched by a subset of the baseline variants\n"\
    "3  : (1 to 1 direct match) find a maximum scoring set of entry pairs such that each entry pair contains"\
    " one query and one baseline variant that result in the same sequence."\
    " In this scheme, different scoring functions and "\
    "matching mode have no difference.\n"

parser.add_argument('-u', '--score_unit', help=score_unit_string, metavar='[-1,0,1]', default=-1)

parser.add_argument('-m', '--match_mode', help=match_mode_string, metavar='[-1,0,1]', default=-1)

parser.add_argument('-s', '--score_scheme', help=score_scheme_string, metavar='[-1,0,1,2,3]', default=-1)

parser.add_argument('-G', '--no_graph', help='disable graphic module', action = 'store_true')

disable_curves_string = "disable Precision-Recall curves, if use -G or --no_graph,"\
                        " then automatically disable these curves"

parser.add_argument('-C', '--disable_curves', help=disable_curves_string, action='store_true')

fast_mode_string = "In this mode, automatically disable graphic module and precision-recall curves,"\
                   " only performs one matching criterion.\n"\
                   " Fast mode is equivalent to use following parameters compulsively: -G -u 0 -m 0 -s 0"

parser.add_argument('-f', '--fast_mode', help=fast_mode_string, action='store_true')

args = parser.parse_args()

if(args.fast_mode):
    args.no_graph = True
    args.score_unit = 0
    args.match_mode = 0
    args.score_scheme = 0


def shell_run(command, hide=False):
    if not RUN:
        time.sleep(3.5)
        print(command)
    else:
        print(command)
        if hide:  # hide output
            FNULL = open(os.devnull, 'w')
            subprocess.call(command, shell=True, stdout=FNULL, stderr=subprocess.STDOUT)
            # subprocess.call(command, shell=True, stdout=FNULL)
            FNULL.close()
        else:
            subprocess.call(command, shell=True)


def check_command(command):
    """
    check if corresponding command available
    """
    if os.path.isfile(command):
        return True

    for cmdpath in os.environ['PATH'].split(':'):
        if os.path.isdir(cmdpath) and command in os.listdir(cmdpath):
            return True
    return False


def table_2_html(table):
    html = '<table border=.5>'
    for i in range(len(table)):
        if i == 0:
            html += '<tr><th>' + '</th><th>'.join(table[i]) + '</th></tr>'
        else:
            html += '<tr><td>' + '</td><td>'.join(table[i]) + '</td></tr>'

    html += '</table>'
    return html

html_head = """
<html>
<head>
<style type="text/css">
table, th, td {
    border: 1px solid black;
    border-collapse: collapse;
}
th, td {
    padding: 5px;
    text-align: left;
}

.box{
    border: 4px solid #ccc;
    padding:20px;
    margin:10px 100px 100px 10px;
}
#advhelp{
    display:none;
}
#advhelp:target{
    display:block;
}
</style>
</head>
<body>
"""

html_tail="""
</body>
</html>
"""

marker_list = ['o', 'v', '1', '8', 's', 'p', '*', 'h', 'x', 'D']

def multiple_compare(baseline_file, query_list, genome_file):
    global check_compare_command
    global output_dir
    if not check_compare_command and not check_command(compare_tool):
        print ('Error: can not find program: ' + compare_tool)
        print ('\t Try "make" command before execute, or contact author for support: ' + author_email)
        exit()
    else:
        check_compare_command = True
    compare_command = compare_tool + ' -b ' + baseline_file + ' -g ' + genome_file + ' -o ' + output_dir

    for query_file in query_list:
        compare_command += ' -q ' + query_file

    if args.thread is not None and int(args.thread) > 0:
        compare_command += ' -t ' + args.thread

    compare_command += ' -u ' + str(args.score_unit) + ' -m ' + str(args.match_mode) + ' -s ' + str(args.score_scheme)

    if args.no_graph or args.disable_curves:
        compare_command += ' -C '

    shell_run(compare_command)


def varmatch_pairwise(baseline_file, query_file, genome_file):
    global output_dir
    ref_basename = os.path.basename(baseline_file)
    que_basename = os.path.basename(query_file)
    output_prefix = output_dir + '/' + ref_basename + '_' + que_basename
    #pairwise_compare(baseline_file, query_file, genome_file)
    return output_prefix

def create_table_prefx(score_unit, match_mode, score_scheme):
    matching_id = ''
    score_unit_string = 'Unit Cost[U]'
    match_mode_string = 'Genotype[G]'
    score_scheme_string = 'Total[T]'
    if(score_unit == '0'):
        matching_id += 'U'
    else:
        matching_id += 'E'
        score_unit_string = 'Edit Distance[E]'

    if match_mode == '0':
        matching_id += 'G'
    else:
        matching_id += 'V'
        match_mode_string = 'Variant[V]'

    if score_scheme == '0':
        matching_id += 'T'
    elif score_scheme == '1':
        matching_id += 'B'
        score_scheme_string = 'Baseline[B]'
    elif score_scheme == '2':
        matching_id += 'Q'
        score_scheme_string = 'Query[Q]'

    return [matching_id, score_unit_string, match_mode_string, score_scheme_string]


def parse_stat(output_prefix):
    global output_dir
    stat_filename = output_dir + '/' + output_prefix + '.stat'
    no_filter_table = []
    head = ['Matching Id', 'Score Unit', 'Match Mode', 'Score Scheme', 'Baseline Match Number', 'Query Match Number', 'Recall(%)', 'Precision(%)']
    match_id = 0
    no_filter_table.append(head)
    x = [] # matching id list
    y = [] # sensitivity list
    z = [] # specificity list

    sensitivity_table = []
    specificity_table = []

    baseline_num = 0.
    query_num = 0.
    with open(stat_filename) as stat_file:
        for line in stat_file.readlines():
            line = line.strip()
            if line.startswith('##'):
                columns = line.split(':')
                if columns[0] == '##Baseline':
                    baseline_num = float(columns[1])
                else:
                    query_num = float(columns[1])
            if line.startswith('#'):
                continue
            match_id += 1
            temp = line.split('\t')
            row = create_table_prefx(temp[0], temp[1], temp[2])
            baseline_match_str_list = temp[4].split(',')
            query_match_str_list = temp[5].split(',')
            query_total_str_list = temp[6].split(',')

            baseline_match_str = baseline_match_str_list[0]
            query_match_str = query_match_str_list[0]

            sensitivity_list = []
            specificity_list = []
            tn_list = []
            for baseline_match_num in baseline_match_str_list:
                try:
                    sensitivity = float(baseline_match_num) * 100 / baseline_num # this is actually recall
                except ZeroDivisionError:
                    sensitivity = 0.0
                    
                sensitivity_list.append(sensitivity)
            #for query_match_num in query_match_str_list:
            #    specificity = float(query_match_num) * 100 / query_num # this is actually precison
            #    specificity_list.append(specificity)
            for i in range(len(query_match_str_list)):
                try:
                    specificity = float(query_match_str_list[i]) * 100 / float(query_total_str_list[i])
                except ZeroDivisionError:
                    specificity = 0.0

                specificity_list.append(specificity)

            x.append(row[0])
            row += [baseline_match_str, query_match_str, "%.3f" % sensitivity_list[0], "%.3f" % specificity_list[0]]

            y.append(sensitivity_list[0])
            z.append(specificity_list[0])

            sensitivity_table.append(sensitivity_list)
            specificity_table.append(specificity_list)
            no_filter_table.append(row)

    return baseline_num, query_num, x, y, z, no_filter_table, sensitivity_table, specificity_table


def create_table_by_matchingid_from_by_query(table_list, matching_list, query_number):
    table_by_matchingid = []
    for matching_index in range(len(matching_list)):
        matching_table = []
        title = ['Query Id', 'Baseline Match Number', 'Query Match Number', 'Recall(%)', 'Precision(%)']
        matching_table.append(title)
        for table_index in range(len(table_list)):
            raw_row = table_list[table_index][matching_index]
            new_row = ['Query' + str(table_index+1)]
            new_row += raw_row[4:]
            matching_table.append(new_row)
        table_by_matchingid.append(matching_table)
    return table_by_matchingid


# all html and picture are created from stat file, not parameters
def create_stat_html(query_list, output_prefix_list):
    global output_dir
    html_filename = output_dir + '/stat.html'
    html_file = open(html_filename, 'w')
    html_file.write(html_head)
    html_file.write('<h1>VarMatch Report</h1>')
    html_file.write('<p>precison and recall analysis for each query with variant quality &ge; 0</p>')
    exp_num = len(output_prefix_list)
    baseline_num_list = []
    query_num_list = []
    table_list = []
    label_list = []
    sensitivity_list = []
    specificity_list = []
    sensitivity_table_list = []
    specificity_table_list = []

    for output_prefix in output_prefix_list:
        (baseline_num, query_num, x, y, z, table, sensitivity_table, specificity_table) = parse_stat(output_prefix)
        baseline_num_list.append(int(baseline_num))
        query_num_list.append(int(query_num))
        label_list.append(x)
        sensitivity_list.append(y)
        specificity_list.append(z)
        table_list.append(table)

        print sensitivity_table
        print specificity_table

        sensitivity_table_list.append(sensitivity_table)
        specificity_table_list.append(specificity_table)

    if(len(table_list)) == 0:
        html_file.close()
        return

    if not args.no_graph:
        import numpy as np
        import matplotlib
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt
        axes = plt.gca()
        #axes.set_xlim([xmin,xmax])
        axes.set_ylim([0,100])
        for i in range(exp_num):
            marker_id = i % len(marker_list)
            marker_sign = marker_list[marker_id]
            label_sign = 'Query ' + str(i+1)
            x = np.array(range(len(label_list[0])))
            plt.xticks(x, label_list[0])
            plt.plot(x, sensitivity_list[i], marker = marker_sign, linestyle = '-', label = label_sign)

        plt.xlabel('Matching Id')
        plt.ylabel('Recall(%)')
        #plt.title('Sensitivity of Queries under Different Matching Parameters')
        plt.legend(loc='best')
        plt.savefig(output_dir + '/sensitivity.png')

        plt.clf() # clear figure for the next
        axes = plt.gca()
        #axes.set_xlim([xmin,xmax])
        axes.set_ylim([0,100])
        for i in range(exp_num):
            marker_id = i % len(marker_list)
            marker_sign = marker_list[marker_id]
            label_sign = 'Query ' + str(i+1)
            x = np.array(range(len(label_list[0])))
            plt.xticks(x, label_list[0])
            plt.plot(x, specificity_list[i], marker = marker_sign, linestyle = '-', label = label_sign)

        plt.xlabel('Matching Id')
        plt.ylabel('Precision(%)')
        #plt.title('Specificity of Queries under Different Matching Parameters')
        plt.legend(loc='best')
        plt.savefig(output_dir + '/specificity.png')

        html_file.write('<h2>VarMatch Matching Parameters Table</h2>'+'\n')
        parameter_table = []
        temp_table = table_list[0]
        for row in temp_table:
            parameter_table.append(row[:4])
        html_file.write(table_2_html(parameter_table))

        html_file.write('<h2>Sensitivity and Specificity of Queries under Different Matching Parameters</h2>'+'\n')
        html_file.write('<p> Baseline File: ' + args.baseline+'</p>' + '\n')
        for i in range(exp_num):
            html_file.write('<p> Query ' + str(i+1) + ': ' + query_list[i] + '</p>' + '\n')
        html_file.write('<h3>Recall of Queries under Different Matching Parameters</h3>'+'\n')
        html_file.write('<img src="sensitivity.png" alt="Sensitivity Graph Not Found...">'+'\n')
        html_file.write('<h3>Precison of Queries under Different Matching Parameters</h3>'+'\n')
        html_file.write('<img src="specificity.png" alt="Specificity Graph Not Found...">'+'\n')

    # sensitivity and specificity analysis by query
    html_file.write('<h2>Sensitivity and Specificity Analysis by Query</h2>'+'\n')

    for i in range(exp_num):
        html_file.write('<div class="box">')
        html_file.write('<h3>Query File: ' + query_list[i] + '</h3>'+'\n')
        html_file.write('<p> Number of Variants in Baseline: ' + str(baseline_num_list[i]) + '</p>'+'\n')
        html_file.write('<p> Number of Variants in Query: ' + str(query_num_list[i]) + '</p>'+'\n')
        html_file.write(table_2_html(table_list[i]))
        html_file.write('</div>'+'\n')

    if exp_num > 1:
        # sensitivity and specificity analysis by matching id
        html_file.write('<h2><Sensitivity and Specificity Analysis by Matching Id/h2>')

        table_by_matchingid = create_table_by_matchingid_from_by_query(table_list, label_list[0], exp_num)
        for i in range(len(label_list[0])):
            html_file.write('<div class="box">')
            html_file.write('<h3>Matching Id: ' + label_list[0][i] + '</h3>'+'\n')
            html_file.write(table_2_html(table_by_matchingid[i]))
            html_file.write('</div>'+'\n')

        html_file.write(html_tail)
        html_file.close()

    # create roc html
    if args.no_graph or args.disable_curves:
        return

    html_filename = output_dir + '/precision_recall.html'
    html_file = open(html_filename, 'w')
    html_file.write(html_head)
    html_file.write('<h1>VarMatch Precision-Recall Curves</h1>')

    html_file.write('<h2>VarMatch Matching Parameters Table</h2>'+'\n')
    parameter_table = []
    temp_table = table_list[0]
    for row in temp_table:
        parameter_table.append(row[:4])
    html_file.write(table_2_html(parameter_table))

    for i in range(exp_num):
        html_file.write('<p>Query ' + str(i+1) + ': ' + query_list[i] + '</p>' + '\n')

    html_file.write('<h2>Precision-Recall Curve by Matching Id</h2>')
    html_file.write('<p>For each matching id, compare all queries in one graph</p>')
    for i in range(len(parameter_table)-1):
        html_file.write('<h3>Precision-Recall Curve for Parameter '+parameter_table[i+1][0]+'</h3>'+'\n')

        plt.clf()

        for j in range(exp_num):
            x = sensitivity_table_list[j][i]
            y = specificity_table_list[j][i]

            x[:] = [a/100 for a in x]
            #y.reverse()
            y[:] = [a/100 for a in y]
            x.sort()
            y.sort(reverse=True)
            y = y[::-1]
            label_sign = 'Query ' + str(j+1)
            plt.plot(x,y, label = label_sign)

        #x = [0.0, 1.0]
        #plt.plot(x, x, linestyle='dashed', color='red', linewidth=2, label='random')

        plt.xlim(0.0, 1.0)
        plt.ylim(0.0, 1.0)
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.legend(loc='best')
        plt.tight_layout()
        plt.savefig(output_dir + '/parameter' + str(i)+'.roc.png')

        html_file.write('<img src="parameter'+str(i) + '.roc.png'+'" alt="ROC Curve Not Found">\n')

    html_file.write('<h2>Precision-Recall Curve by Query</h2>')
    html_file.write('<p>For each query, compare all matching id in one graph</p>')
    for i in range(exp_num):
        html_file.write('<h3>Precision-Recall Curve for Query '+str(i+1)+'</h3>'+'\n')

        plt.clf()

        colormap = plt.cm.gist_ncar
        plt.gca().set_color_cycle([colormap(k) for k in np.linspace(0, 0.9, len(parameter_table))])

        for j in range(len(parameter_table)-1):
            x = sensitivity_table_list[i][j]
            y = specificity_table_list[i][j]

            #x[:] = [1.0 - a/100 for a in x]
            #y.reverse()
            #y[:] = [a/100 for a in y]

            #x.sort()
            #y.sort(reverse=True)

            label_sign = parameter_table[j+1][0]
            plt.plot(x,y, label = label_sign)

        #x = [0.0, 1.0]
        #plt.plot(x, x, linestyle='dashed', color='red', linewidth=2, label='random')

        plt.xlim(0.0, 1.0)
        plt.ylim(0.0, 1.0)
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.legend(loc='best')
        plt.tight_layout()
        plt.savefig(output_dir + '/query' + str(i)+'.roc.png')

        html_file.write('<img src="query'+str(i) + '.roc.png'+'" alt="ROC Curve Not Found">\n')

    html_file.write(html_tail)
    html_file.close()



def main():
    if len(sys.argv) < 2:
        parser.print_help()
        exit()

    global check_compare_command
    global script_path
    global compare_tool
    global output_dir
    global temp_dir

    check_compare_command = True

    script_path = sys.path[0]
    compare_tool = script_path + '/src/vm-core'
    output_dir = ''
    temp_dir = ''

    # create output directory
    if args.output is None or args.output == '':
        output_dir = os.getcwd() + '/output'
    else:
        output_dir = args.output
    if output_dir == '':
        output_dir = os.getcwd() + '/output'
    if not os.path.exists(output_dir):
        os.mkdir(output_dir)

    temp_dir = output_dir + '/temp'

    query_list = args.query

    multiple_compare(args.baseline, query_list, args.genome)

    if args.score_scheme == '3':
        exit()
        
    output_prefix_list = []
    for i in range(len(query_list)):
        output_prefix_list.append('query'+str(i+1))

    create_stat_html(query_list, output_prefix_list)

if __name__ == '__main__':
    main()
